<TeXmacs|1.0.7.21>

<style|compact>

<\body>
  <doc-data|<doc-title|STAT 535 Final Project
  Report>|<doc-author|<author-data|<author-name|Anh Huynh>>>>

  <abstract-data|<abstract|We report on application of two supervised
  learning models to the task of recognizing written digits. The first model
  is the <math|k>-nearest-neighbor, and the second one is the Gaussian
  discriminant analysis. We describe the feature set used, as well as
  complete model description and parameterization. We explain the training
  strategy and provide experimental results, which include the learning curve
  and validation losses for both models. Lastly, an estimate of the
  classification error is given. The best classification error attained by
  the <math|k>-nearest-neighbor is 5.75%, whereas for the Gaussian
  discriminant analysis, it is <math|15.5>%. The second method, however, is
  markedly faster than the first one. >>

  <section|Project Objective>

  The goal of this project is to use two supervised learning models, namely,
  <math|k>-nearest-neighbor and Gaussian discriminant analysis, to learn the
  task of recognizing written digits. The available data include two sets
  <samp|trainingImagesA.npy> and <samp|trainingImagesB.npy> of 20000 images
  each. The images have size <math|28\<times\>28> and pixel-range between 0
  and 255. The labels to these images, which explains which digit the image
  is supposed to depict, are given in two sets <samp|trainingLabelA.npy> and
  <samp|trainingLabelB.npy>.

  <section|Preprocessing>

  We can regard each image in the training set as a
  <math|28<rsup|2>=784>-dimensional vector, whose components have range
  <math|<around*|[|0,255|]>>. As is famously known in the case of the
  <math|k>-nearest-neighbor model, at this scale, the curse of dimensionality
  is too great for the model to work well. We therefore seeked to reduce the
  dimensions naively as follow. We took <math|7\<times\>7> subsquares within
  the <math|28\<times\>28> image and computed the average value of the pixels
  within those <math|7\<times\>7> subsquares. To be precise, we used the
  subsquares whose sides project onto the axes as one of the following
  intervals:\ 

  <\equation*>
    <around*|[|0,6|]>,<around*|[|4\<nocomma\>,10|]>,<around*|[|7,13|]>,<around*|[|10,16|]>,<around*|[|14,20|]>,<around*|[|17,23|]>,<around*|[|21,27|]>.
  </equation*>

  Thus, the <math|28\<times\>28> images are reduced to <math|7\<times\>7>
  images, or <math|49>-dimensional vectors. In our experience this turns out
  to be adequate.\ 

  Illustrations.

  The reason for choosing this feature set is as follows. First of all, even
  though this reduction blurs the images somewhat, it still preserves the
  relative position of the brighter pixels versus the darker pixels.
  Furthermore, observe that the area closed to the boundary of the images are
  invariably black. In our reduction of the images, there is a certain amount
  of overlapping of the subsquares on the middle part of the images, thus
  giving more weights to the more relevant part of the images. In Section
  <reference|result>, we will detail the effect of using the original images,
  using nonoverlapping <math|4\<times\>4> subsquares, and using overlapping
  <math|7\<times\>7> subsquares.\ 

  In the case of the Gaussian discriminant analysis model, we are faced with
  a different difficulty. Here we need to compute the covariance matrices and
  their determinant. It often turns out that with the original images, the
  determinants are too big for numerical computation. Therefore, we used the
  same image reduction procedure as for the <math|k>-nearest-neighbor model.
  Furthermore, we divide the value of every pixel in the picture by <math|4>.
  This preserves the relative different between pixels, but reduce the value
  of the determinants that we need to compute. In our experience, a factor of
  <math|4> is sufficient for our classification task.\ 

  <section|Predictors>

  complete model description, parametrization

  \;

  <section|Training Algorithms>

  what algorithm, what parameters, anything unusual you did. Do not reproduce
  the algorithms from the book or lecture unless you make modifications.

  \;

  <section|Training Strategy>

  Reproducible description of what you did for training (e.g training set
  sizes, number epochs, how initialized, did you do CV or model selection)

  \;

  \;

  <section|Experimental Results><label|result>

  learning curve(s), training (validation) losses, estimated parameter values
  if they are interpretable and plottable. Be selectivein what you show!
  Credit will given for careful analysis or visualization of the results.

  \;

  \;

  <section|Classification Error>

  Estimate of the classification error L. Optionally, an interval [Lmin,
  Lmax] where you believe L will be, and how you estimated these.

  \;

  \;

  \;

  \;

  Ideas to add to project

  - Use a third method to classify 2 numbers that are hardest to distinguish

  - Plot training errors and generalization errors with different params like
  how many dimensions I keep, PCA, or number of neighbors... explain the
  curve. like more dimension, less accurate, but than too few dimension
  =\<gtr\> information loss less accurate as well

  \;

  Friday, 11/15/13.\ 

  Used overlapping reduction to 7x7 matrices. Use entire img_A as training
  set, and tested on entire img_B.\ 

  Time = 6474 seconds

  Error = 0.05755%

  \;
</body>

<\initial>
  <\collection>
    <associate|dpi|1200>
    <associate|font-base-size|12>
    <associate|page-bot|2cm>
    <associate|page-odd|2cm>
    <associate|page-right|2cm>
    <associate|page-top|2cm>
    <associate|page-type|letter>
    <associate|par-par-sep|1fn>
    <associate|par-sep|0.5fn>
  </collection>
</initial>

<\references>
  <\collection>
    <associate|auto-1|<tuple|1|?>>
    <associate|auto-2|<tuple|2|?>>
    <associate|auto-3|<tuple|3|?>>
    <associate|auto-4|<tuple|4|?>>
    <associate|auto-5|<tuple|5|?>>
    <associate|auto-6|<tuple|6|?>>
    <associate|auto-7|<tuple|7|?>>
    <associate|report|<tuple|<with|mode|<quote|math>|\<bullet\>>|?>>
    <associate|result|<tuple|6|?>>
  </collection>
</references>

<\auxiliary>
  <\collection>
    <\associate|toc>
      <vspace*|1fn><with|font-series|<quote|bold>|math-font-series|<quote|bold>|1<space|2spc>Project
      Objective> <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-1><vspace|0.5fn>

      <vspace*|1fn><with|font-series|<quote|bold>|math-font-series|<quote|bold>|2<space|2spc>Preprocessing>
      <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-2><vspace|0.5fn>

      <vspace*|1fn><with|font-series|<quote|bold>|math-font-series|<quote|bold>|3<space|2spc>Predictors>
      <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-3><vspace|0.5fn>

      <vspace*|1fn><with|font-series|<quote|bold>|math-font-series|<quote|bold>|4<space|2spc>Training
      Algorithms> <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-4><vspace|0.5fn>

      <vspace*|1fn><with|font-series|<quote|bold>|math-font-series|<quote|bold>|5<space|2spc>Training
      Strategy> <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-5><vspace|0.5fn>

      <vspace*|1fn><with|font-series|<quote|bold>|math-font-series|<quote|bold>|6<space|2spc>Experimental
      Results> <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-6><vspace|0.5fn>

      <vspace*|1fn><with|font-series|<quote|bold>|math-font-series|<quote|bold>|7<space|2spc>Classification
      Error> <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-7><vspace|0.5fn>
    </associate>
  </collection>
</auxiliary>